{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1556326,"sourceType":"datasetVersion","datasetId":918769}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T04:46:09.500980Z","iopub.execute_input":"2024-11-10T04:46:09.501489Z","iopub.status.idle":"2024-11-10T04:46:15.508160Z","shell.execute_reply.started":"2024-11-10T04:46:09.501442Z","shell.execute_reply":"2024-11-10T04:46:15.506648Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class VOCDataset(torch.utils.data.Dataset):\n    def __init__(self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transform=None):\n        self.annotations = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.label_dir = label_dir\n        self.S = S\n        self.B = B\n        self.C = C\n    def __len__(self):\n        return len(self.annotations)\n    def __getitem__(self, index):\n        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n        boxes = []\n        with open(label_path) as f:\n            for label in f.readlines():\n                class_label, x, y, width, height = [\n                    float(x) if float(x) != int(float(x)) else int(x)\n                    for x in label.replace(\"\\n\", \"\").split()\n                ]\n                boxes.append([class_label, x, y, width, height])\n        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n        image = Image.open(img_path)\n        boxes = torch.tensor(boxes)\n        if self.transform:\n            image, boxes = self.transform(image, boxes)\n        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n        for box in boxes:\n            class_label, x, y, width, height = box.tolist()\n            class_label = int(class_label)\n            \n            i,j = int(self.S * y), int(self.S * x)\n            x_cell, y_cell = self.S * x - j, self.S * y - i\n            \n            width_cell, height_cell = (width * self.S, height * self.S)\n            \n            if label_matrix[i,j,20] == 0:\n                label_matrix[i,j,20] = 1\n                box_coordinates = torch.tensor(\n                    [x_cell, y_cell, width_cell, height_cell]\n                )\n                label_matrix[i,j,21:25] = box_coordinates\n                \n                label_matrix[i,j,class_label] = 1\n        return image, label_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T04:46:15.510562Z","iopub.execute_input":"2024-11-10T04:46:15.511270Z","iopub.status.idle":"2024-11-10T04:46:15.530108Z","shell.execute_reply.started":"2024-11-10T04:46:15.511216Z","shell.execute_reply":"2024-11-10T04:46:15.528483Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class YoloLoss(nn.Module):\n    def __init__(self, S=7, B=2, C=20):\n        super(YoloLoss, self).__init__()\n        self.mse = nn.MSELoss(reduction=\"sum\")\n        self.S = S\n        self.B = B\n        self.C = C\n        \n        self.lambda_noobj = 0.5\n        self.lambda_coord = 5\n        \n    def forward(self, predictions, target):\n        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n        \n        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n        iou_b2 = intersection_over_union(predictions[...,26:30], target[...,21:25])\n        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim = 0)\n        \n        iou_maxes, bestbox = torch.max(ious, dim = 0)\n        exists_box = target[...,20].unsqueeze(3)\n        \n        box_predictions = exists_box * ((bestbox * predictions[..., 26: 30]) + (1 - bestbox) * predictions[..., 21:25])\n        box_targets = exists_box * target[..., 21:25]\n        \n        \n        #box corrdinate loss\n        \n        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(torch.abs(box_predictions[..., 2:4]+ 1e-6))\n        box_targets[..., 2:4] =  torch.sqrt(box_targets[..., 2:4])\n        box_loss = self.mse(\n            torch.flatten(box_predictions, end_dim=-2), \n            torch.flatten(box_targets, end_dim=-2),\n        )\n        \n        #Confidence Object loss\n        \n        pred_box = (bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21])\n        object_loss = self.mse(\n            torch.flatten(exists_box * pred_box),\n            torch.flatten(exists_box * target[..., 20:21])\n        )\n        \n        #Confidence No Object loss\n        \n        no_object_loss = self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim = 1 ),\n            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim = 1)\n        )\n        \n        no_object_loss += self.mse(\n            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim = 1),\n            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim = 1)\n        )\n        \n        # class loss\n        class_loss = self.mse(\n            torch.flatten(exists_box * predictions[..., :20], end_dim=-2),\n            torch.flatten(exists_box * target[..., 20], end_dim=-2)\n        )\n        \n        loss = (\n            self.lambda_coord * box_loss + \n            object_loss +\n            self.lambda_noobj * no_object_loss +\n            class_loss\n        )\n        \n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T04:46:15.531847Z","iopub.execute_input":"2024-11-10T04:46:15.532238Z","iopub.status.idle":"2024-11-10T04:46:15.561608Z","shell.execute_reply.started":"2024-11-10T04:46:15.532199Z","shell.execute_reply":"2024-11-10T04:46:15.560450Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"architecture_config = [\n    (7, 64, 2, 3),\n    \"M\",\n    (3, 192, 1, 1),\n    \"M\",\n    (1, 128, 1, 0),\n    (3, 256, 1, 1),\n    (1, 256, 1, 0),\n    (3, 512, 1, 1),\n    \"M\",\n    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n    (1, 512, 1, 0),\n    (3, 1024, 1, 1),\n    \"M\",\n    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n    (3, 1024, 1, 1),\n    (3, 1024, 2, 1),\n    (3, 1024, 1, 1),\n    (3, 1024, 1, 1),\n]\n\nclass CNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(CNNBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias = False, **kwargs)\n        self.batchnorm = nn.BatchNorm2d(out_channels)\n        self.leakyrelu = nn.LeakyReLU(0.1)\n        \n    def forward(self, x):\n        return self.leakyrelu(self.batchnorm(self.conv(x)))\n\n\nclass YoloV1(nn.Module):\n    def __init__(self, in_channels=3, **kwargs):\n        super(YoloV1, self).__init__()\n        self.architecture = architecture_config\n        self.in_channels = in_channels\n        self.darknet = self._create_conv_layers(self.architecture)\n        self.fcs = self._create_fcs(**kwargs)\n    \n    def forward(self, x):\n        x = self.darknet(x)\n        return self.fcs(torch.flatten(x,start_dim=1))\n    \n    def _create_conv_layers(self, architecture):\n        layers = []\n        in_channels = self.in_channels\n        \n        for x in architecture:\n            if type(x) == tuple:\n                layers += [\n                    CNNBlock(\n                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3]\n                    )\n                ]\n                in_channels = x[1]\n            elif type(x) == str:\n                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n            elif type(x) == list:\n                conv1 = x[0]\n                conv2 = x[1]\n                num_repeats = x[2]\n                \n                for _ in range(num_repeats):\n                    layers += [\n                        CNNBlock(\n                            in_channels,\n                            conv1[1],\n                            kernel_size=conv1[0],\n                            stride=conv1[2],\n                            padding=conv1[3]\n                        )\n                    ]\n                    \n                    layers += [\n                        CNNBlock(\n                            conv1[1],\n                            conv2[1],\n                            kernel_size=conv2[0],\n                            stride=conv2[2],\n                            padding=conv2[3]\n                        )\n                    ]\n                    \n                    in_channels = conv2[1]\n                \n        return nn.Sequential(*layers)\n    \n    \n    def _create_fcs(self, split_size, num_boxes, num_classes):\n        S, B, C = split_size, num_boxes, num_classes\n        \n        return nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(1024 * S * S, 496),\n            nn.Dropout(0.0),\n            nn.LeakyReLU(0.1),\n            nn.Linear(496, S*S*(C+B*5))\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T04:46:15.563893Z","iopub.execute_input":"2024-11-10T04:46:15.564288Z","iopub.status.idle":"2024-11-10T04:46:15.586229Z","shell.execute_reply.started":"2024-11-10T04:46:15.564245Z","shell.execute_reply":"2024-11-10T04:46:15.584771Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n    if box_format == \"midpoint\":\n        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n\n    if box_format == \"corners\":\n        box1_x1 = boxes_preds[..., 0:1]\n        box1_y1 = boxes_preds[..., 1:2]\n        box1_x2 = boxes_preds[..., 2:3]\n        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n        box2_x1 = boxes_labels[..., 0:1]\n        box2_y1 = boxes_labels[..., 1:2]\n        box2_x2 = boxes_labels[..., 2:3]\n        box2_y2 = boxes_labels[..., 3:4]\n\n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n\n    # .clamp(0) is for the case when they do not intersect\n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n\n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n\n    return intersection / (box1_area + box2_area - intersection + 1e-6)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T04:46:15.588257Z","iopub.execute_input":"2024-11-10T04:46:15.588769Z","iopub.status.idle":"2024-11-10T04:46:15.610085Z","shell.execute_reply.started":"2024-11-10T04:46:15.588722Z","shell.execute_reply":"2024-11-10T04:46:15.607772Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"seed = 123\ntorch.manual_seed(seed)\n\nEPOCH = 1000\nLEARNING_RATE = 2e-5\nDEVICE = torch.device(\"cpu\")\nBATCH_SIZE = 16\nWEIGHT_DECAY = 0\nNUM_WORKERS = 2\nLOAD_MODEL = False\nPIN_MEMORY = True\nLOAD_MODEL_FILE = \"overfit.pth.tar\"\nIMG_DIR = \"/kaggle/input/pascalvoc-yolo/images\"\nLABEL_DIR = \"/kaggle/input/pascalvoc-yolo/labels\"\nCSV_DIR_TRAIN = \"/kaggle/input/pascalvoc-yolo/100examples.csv\"\nCSV_DIR_TEST = \"/kaggle/input/pascalvoc-yolo/test.csv\"\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n    \n    def __call__(self, img, bboxes):\n        for t in self.transforms:\n            img, bboxes = t(img), bboxes\n        \n        return img, bboxes\n\ntransform = Compose([transforms.Resize((448,448)), transforms.ToTensor()])\n\ndef train_fn(train_loader, model, optimizer, loss_fn):\n    loop = tqdm(train_loader, leave=True)\n    mean_loss = []\n    for batch_idx, (x, y) in enumerate(loop):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        out = model(x)\n        loss = loss_fn(out, y)\n        mean_loss.append(loss.item())\n        \ndef main():\n    model = YoloV1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n    optimizer = optim.Adam(\n        model.parameters(), lr = LEARNING_RATE, weight_decay=WEIGHT_DECAY,\n    )\n    \n    loss_fn = YoloLoss()\n    \n    if LOAD_MODEL:\n        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n    \n    train_dataset = VOCDataset(\n        CSV_DIR_TRAIN, \n        transform=transform,\n        img_dir = IMG_DIR, \n        label_dir= LABEL_DIR\n    )\n    test_dataset = VOCDataset(\n        CSV_DIR_TEST, \n        transform=transform, \n        img_dir=IMG_DIR, \n        label_dir=LABEL_DIR,\n    )\n    train_loader = DataLoader(\n        dataset = train_dataset,\n        batch_size = BATCH_SIZE,\n        num_workers = NUM_WORKERS,\n        pin_memory = PIN_MEMORY,\n        shuffle = True,\n        drop_last = True\n    )\n    # for epoch in range(EPOCH):\n    #     pred_boxes, target_boxes = get_bboxes(\n    #         train_loader,\n    #         model,\n    #         iou_threshold = 0.5,\n    #         threshold = 0.4,\n    #     )\nmain()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T04:46:15.612072Z","iopub.execute_input":"2024-11-10T04:46:15.612568Z","iopub.status.idle":"2024-11-10T04:46:17.063154Z","shell.execute_reply.started":"2024-11-10T04:46:15.612523Z","shell.execute_reply":"2024-11-10T04:46:17.061825Z"}},"outputs":[],"execution_count":6}]}